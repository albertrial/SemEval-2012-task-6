{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT PROJECT - SEMANTIC TEXTUAL SIMILARITY (USING INFERSENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students:\n",
    "   - Albert Rial\n",
    "   - Utku Ünal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the one with the implementation of the system that uses the pre-trained model called Infersent. The main one is the other notebook. Here we will only detail the changes done in this particular system and comment the conclusions. The rest of comments and conclusions are in the other notebook.\n",
    "\n",
    "The main functions added are:\n",
    "- **load_infersent**: given a list of sentences it builds the infersent model using their words and the 1000000 most common English words. InferSent is a sentence embeddings method that provides semantic representations for English sentences.\n",
    "- **sentence_emb_similarity**: given two sentences and using the infersent pre-trained sentence encoder (sentence embedding) from Facebook it encodes the sentences and computes the euclidean distance between the vectors obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from InferSent.models import InferSent\n",
    "\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('words')\n",
    "#nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA AND PREPROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the functions that load the data (sentences and gold-standard) from the files and preprocess them in order to tokenize the words, remove punctuation and stopwords, get the lemmas, etc. These functions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_gs**: reads the gold-standard scores from a set of files and returns a list with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gs(file):\n",
    "    gs = []\n",
    "    gs_files = glob.glob(file)\n",
    "    for name in gs_files:\n",
    "        with open(name, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                score = line.strip().split('\\t')\n",
    "                gs.append(float(score[0]))\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_sentences**: reads the pairs of sentences from a set of files and returns two lists with the sentences. The first list contains the first sentence of each pair and the second list the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    \n",
    "    input_files = glob.glob(file)\n",
    "    \n",
    "    for name in input_files:\n",
    "        with open(name, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                # Split\n",
    "                pair_of_sentences = line.strip().split('\\t')\n",
    "\n",
    "                # Tokenize\n",
    "                sentences1.append(pair_of_sentences[0])\n",
    "                sentences2.append(pair_of_sentences[1])\n",
    "\n",
    "    return sentences1, sentences2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_all_words**: given a sentence, returns its words in lower case, without taking into account the following punctuation characters !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(sentence):\n",
    "    return [word.lower() for word in nltk.word_tokenize(sentence) if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_words**: like the previous function, given a sentence, returns its words in lower case. In this case apart from removing the punctuation we also remove the english stopwords (‘the’, ‘is’, ‘are’, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence):\n",
    "    return [word.lower() for word in nltk.word_tokenize(sentence) if word not in string.punctuation and word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_tag**: function that transform a POS tag given by the NLTK POS-tagger to the format that WordNetLemmatizer and Lesk function can undersantd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tag(tag):\n",
    "    if tag[0] in {'N', 'V', 'R'}:\n",
    "        return tag[0].lower()\n",
    "    elif tag[0] in {'J'}:\n",
    "        return 'a'\n",
    "    else:\n",
    "        return tag[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_lemmas**: given a list with pairs of (word, pos_tag) it returns a list with the lemmas of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(pos_tags):\n",
    "    lemmas = []\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for pos_tag in pos_tags:\n",
    "        word = pos_tag[0].lower()\n",
    "        tag = transform_tag(pos_tag[1])\n",
    "        \n",
    "        if tag in {'n', 'v', 'r', 'a'}:\n",
    "            lemmas.append(wnl.lemmatize(word, pos=tag))\n",
    "        else:\n",
    "            lemmas.append(word)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_word_importance**: given a list of sentences computes the frequency of each word in all the sentences and returns a dictionary with the importance of each word (considering as importance, the total number of words divided by the frequency of each word). We consider that the words with more meaning are the ones that appear less in the corpus. We use the log in the division in order to have numbers of less magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_importance(sentences):\n",
    "    freq = FreqDist()\n",
    "    total_freq = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        all_words = get_all_words(sentence)\n",
    "        for word in all_words:\n",
    "            freq[word.lower()] += 1\n",
    "            total_freq += 1\n",
    "                    \n",
    "    importance = {}\n",
    "    for word in freq.keys():\n",
    "        importance[word] = math.log(float(total_freq) / float(freq[word]))\n",
    "                    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load_infersent**: given a list of sentences it builds the infersent model using their words and the 1000000 most common English words. InferSent is a sentence embeddings method that provides semantic representations for English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_infersent(sentences):\n",
    "    MODEL_PATH = 'encoder/infersent2.pkl'\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
    "    infSent = InferSent(params_model)\n",
    "    infSent.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "    W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "    infSent.set_w2v_path(W2V_PATH)\n",
    "\n",
    "    # Build infersent\n",
    "    infSent.build_vocab_k_words(K=1000000)\n",
    "    infSent.update_vocab(sentences, tokenize=True)\n",
    "    \n",
    "    return infSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define all our features. We have several functions that given two sentences, words or lemmas return a float representing a feature of similarity between them. We have both lexical and syntactic features, however not all of them have finally been used because of its performance.\n",
    "The features defined are the ones below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**jaccard_similarity**: given two lists of words the function computes the jaccard similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(words1, words2):\n",
    "    return 1-jaccard_distance(set(words1), set(words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cosine_similarity**: given two sets of words we compute the cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(words1_set, words2_set):\n",
    "    l1 =[];l2 =[] \n",
    "    rvector = words1_set.union(words2_set)  \n",
    "    for w in rvector: \n",
    "        if w in words1_set: l1.append(1)\n",
    "        else: l1.append(0) \n",
    "        if w in words2_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "    # Cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**synsets_similarity**: in this function first, we receive two lists of lemmas and a method for computing similarity between synsets (path, lch, wup, lin). For each lemma in the first list, we calculate the maximum synset similarity between all the lemmas in the second list. With the maximum similarites obtained we compute the mean similarity between the two lists of lemmas. Moreover, as computing the wordnet similarities between one lemma in respect to other is not the same as doing it  inversely, we compute two means.\n",
    "\n",
    "As one lemma has more than one synsets, to compute the synset similarity between one lemma and another we use the function *max_similarity_synsets*. This function receives two lemmas, gets the synsets of each one of them, and for each synset of one lemma computes the similarity between the synsets of the other lemma and returns the maximum similarity. To compute the similarity between synsets it uses *wordnet_similarity*, that is a simple function that calls the wordnet similarity functions given two synsets and the specific method to use.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_synsets_sim = {}\n",
    "\n",
    "def wordnet_similarity(s1, s2, method):\n",
    "    if method == \"path\" and s1 is not None and s2 is not None:\n",
    "        return s1.path_similarity(s2)\n",
    "    \n",
    "    elif method == \"lch\" and s1 is not None and s2 is not None and s1.pos == s2.pos:\n",
    "        return s1.lch_similarity(s2)\n",
    "    \n",
    "    elif method == \"wup\" and s1 is not None and s2 is not None:\n",
    "        return s1.wup_similarity(s2)\n",
    "    \n",
    "    elif method == \"lin\" and s1 is not None and s2 is not None and s1.pos == s2.pos and s1.pos in {'n', 'v', 'r', 'a'}:\n",
    "        return s1.lin_similarity(s2)\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def max_similarity_synsets(l1, l2, method):\n",
    "    if l1 == l2:\n",
    "        if method == \"lch\":\n",
    "            return 3\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    elif (l1,l2,method) in computed_synsets_sim:\n",
    "        return computed_synsets_sim[(l1,l2,method)]\n",
    "    \n",
    "    synsets1 = wordnet.synsets(l1)\n",
    "    synsets2 = wordnet.synsets(l2)\n",
    "    \n",
    "    similarities = []\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            similarity = wordnet_similarity(s1, s2, method)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "            \n",
    "    if len(similarities) > 0:\n",
    "        computed_synsets_sim[(l1,l2,method)] = max(similarities)\n",
    "        return max(similarities)\n",
    "    else:\n",
    "        computed_synsets_sim[(l1,l2,method)] = 0\n",
    "        return 0\n",
    "\n",
    "def synsets_similarity(lemmas1, lemmas2, method):\n",
    "    sum_sim1 = 0\n",
    "    for l1 in lemmas1:\n",
    "        sum_sim1 += max([max_similarity_synsets(l1, l2, method) for l2 in lemmas2])\n",
    "    mean_sim1 = sum_sim1 / len(lemmas1)\n",
    "    \n",
    "    sum_sim2 = 0\n",
    "    for l2 in lemmas2:\n",
    "        sum_sim2 += max([max_similarity_synsets(l2, l1, method) for l1 in lemmas1])\n",
    "    mean_sim2 = sum_sim2 / len(lemmas2)\n",
    "    \n",
    "    if mean_sim1 > 0 or mean_sim2 > 0:\n",
    "        return (2 * mean_sim1 * mean_sim2)/(mean_sim1+mean_sim2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**same_num_entities**: this is a very basic function that, given two lists of name entities, counts for each list the number of entities of an specific label and returns 1 if both have the same number of them, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_num_entities(ne1, ne2, entity):\n",
    "    num1 = 0 \n",
    "    for p1 in ne1:\n",
    "        if isinstance(p1, nltk.tree.Tree) and p1.label()==entity:\n",
    "            num1 += 1\n",
    "            \n",
    "    num2 = 0    \n",
    "    for p2 in ne2:\n",
    "        if isinstance(p2, nltk.tree.Tree) and p2.label()==entity:\n",
    "            num2 += 1\n",
    "        \n",
    "    if num1 == num2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sentiment_similarity**: given two lists of lemmas it computes the polarity of each list summing the polarity of each lemma, and then computes the absolute difference between them (it is normalized using the maximum polarity). To calculate the polarity of each lemma uses the function *get_sentiment_score*, which uses the SentiWordnet to get the positive score and negative score of each synset of the lemma, and sum all to get the polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(lemma):\n",
    "    synsets = wordnet.synsets(lemma)\n",
    "    score = 0\n",
    "    for s in synsets:\n",
    "        senti_synset = sentiwordnet.senti_synset(s.name())\n",
    "        if senti_synset is not None:\n",
    "            score += senti_synset.pos_score() - senti_synset.neg_score()\n",
    "    return score\n",
    "    \n",
    "def sentiment_similarity(lemmas1, lemmas2):\n",
    "    polarity1 = 0\n",
    "    for l1 in lemmas1:\n",
    "        polarity1 += get_sentiment_score(l1)\n",
    "        \n",
    "    polarity2 = 0\n",
    "    for l2 in lemmas2:\n",
    "        polarity2 += get_sentiment_score(l2)\n",
    "    \n",
    "    if polarity1 > 0 or polarity2 > 0:\n",
    "        return abs(polarity1-polarity2) / max(polarity1, polarity2)\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lesk_similarity**: given two lists of words we apply lesk algorithm to do word sense disambiguation, and we compute the jaccard similarity between the senses obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_similarity(words1, words2):\n",
    "    pos_tags1 = pos_tag(words1)\n",
    "    pos_tags2 = pos_tag(words2)\n",
    "    \n",
    "    lesk_synsets1 = []\n",
    "    for i in range(0, len(words1)):\n",
    "        if(pos_tags1[i] in {'n', 'v', 'r', 'a'}):\n",
    "            lesk_synsets1.append(lesk(words1, words1[i], pos_tags1[i]))\n",
    "            \n",
    "    lesk_synsets2 = []\n",
    "    for i in range(0, len(words2)):\n",
    "        if(pos_tags2[i] in {'n', 'v', 'r', 'a'}):\n",
    "            lesk_synsets2.append(lesk(words2, words2[i], pos_tags2[i]))\n",
    "    \n",
    "    if len(lesk_synsets1) > 0 and len(lesk_synsets2) > 0:\n",
    "        return 1-jaccard_distance(set(lesk_synsets1), set(lesk_synsets2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**length_difference**: given two list of words it returns the difference between their lengths (normalized using the maximum length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_difference(words1, words2):\n",
    "    return abs(len(words1)-len(words2)) / max(len(words1), len(words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unigram_similarity**: given two lists of words it counts the number of same words that we have in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_similarity(words1, words2):\n",
    "    count_same = 0\n",
    "    for w in words1:\n",
    "        count_same += min(words1.count(w), words2.count(w))\n",
    "    \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count_same/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unigram_similarity_importance**: given two lists of words it counts the number of same words that we have in both lists but when counting a word we take into account its importance in the corpus. This important is computed using previous function *get_word_importance* and stored in the dictionary *word_importance*. The less a word appears in the corpus the more important we consider it, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_similarity_importance(words1, words2):\n",
    "    count_same = 0\n",
    "    for w in words1:\n",
    "        count_same += min(words1.count(w), words2.count(w)) * word_importance.get(w, max_importance)\n",
    "        \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count_same/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bigram_similarity**: given two lists of words this function counts the number of same bigrams that we have in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_similarity(words1, words2):\n",
    "    finder1 = BigramCollocationFinder.from_words(words1)\n",
    "    finder2 = BigramCollocationFinder.from_words(words2)\n",
    "    \n",
    "    bigrams1 = []\n",
    "    freq1 = []\n",
    "    for b1 in finder1.ngram_fd.items():\n",
    "        bigrams1.append(b1[0])\n",
    "        freq1.append(b1[1])\n",
    "        \n",
    "    bigrams2 = []\n",
    "    freq2 = []\n",
    "    for b2 in finder2.ngram_fd.items():\n",
    "        bigrams2.append(b2[0])\n",
    "        freq2.append(b2[1])\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(bigrams1)):\n",
    "        if bigrams1[i] in bigrams2:\n",
    "            count += min(freq1[i], freq2[bigrams2.index(bigrams1[i])])\n",
    "            \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trigram_similarity**: given two lists of words this function counts the number of same trigrams that we have in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_similarity(words1, words2):\n",
    "    finder1 = TrigramCollocationFinder.from_words(words1)\n",
    "    finder2 = TrigramCollocationFinder.from_words(words2)\n",
    "    \n",
    "    trigrams1 = []\n",
    "    freq1 = []\n",
    "    for t1 in finder1.ngram_fd.items():\n",
    "        trigrams1.append(t1[0])\n",
    "        freq1.append(t1[1])\n",
    "        \n",
    "    trigrams2 = []\n",
    "    freq2 = []\n",
    "    for t2 in finder2.ngram_fd.items():\n",
    "        trigrams2.append(t2[0])\n",
    "        freq2.append(t2[1])\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(trigrams1)):\n",
    "        if trigrams1[i] in trigrams2:\n",
    "            count += min(freq1[i], freq2[trigrams2.index(trigrams1[i])])\n",
    "            \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sentence_emb_similarity**: given two sentences and using the infersent pre-trained sentence encoder (sentence embedding) from Facebook it encodes the sentences and computes the euclidean distance between the vectors obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_emb_similarity(sentence1, sentence2):\n",
    "    eb1 = infersent.encode([sentence1], tokenize=True)\n",
    "    eb2 = infersent.encode([sentence2], tokenize=True)\n",
    "    return euclidean(eb1,eb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call previous functions to preprocess sentences and compute features\n",
    "In the following function we receive two sentences and using the first functions defined in this notebook we preprocess them, in order to get all words, words without stopwords, POS tags, lemmas and NEs. Once preprocessed, we compute the features between each pair of sentences and we append the features to our features array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sentences1, sentences2):\n",
    "    features = []\n",
    "    for i in range(len(sentences1)):\n",
    "        sentence1 = sentences1[i]\n",
    "        sentence2 = sentences2[i]\n",
    "        \n",
    "        # Get all words\n",
    "        all_words1 = get_all_words(sentence1)\n",
    "        all_words2 = get_all_words(sentence2)\n",
    "        \n",
    "        # Get words without stopwords\n",
    "        words1 = get_words(sentence1)\n",
    "        words2 = get_words(sentence2)\n",
    "        \n",
    "        # POS tags\n",
    "        pos_tags1 = pos_tag(words1)\n",
    "        pos_tags2 = pos_tag(words2)\n",
    "\n",
    "        # Lemmas\n",
    "        lemmas1 = get_lemmas(pos_tags1)\n",
    "        lemmas2 = get_lemmas(pos_tags2)\n",
    "\n",
    "        # Name entities\n",
    "        ne1 = ne_chunk(pos_tags1)\n",
    "        ne2 = ne_chunk(pos_tags2)\n",
    "        \n",
    "        # Features\n",
    "        features.append([jaccard_similarity(lemmas1, lemmas2),\n",
    "                         jaccard_similarity(words1, words2),\n",
    "                         #jaccard_similarity(all_words1, all_words2),\n",
    "                         cosine_similarity(set(words1),set(words2)),\n",
    "                         #cosine_similarity(set(all_words1),set(all_words2)),\n",
    "                         cosine_similarity(set(lemmas1),set(lemmas2)),\n",
    "                         #synsets_similarity(lemmas1, lemmas2, \"lch\"),\n",
    "                         synsets_similarity(lemmas1, lemmas2, \"path\"),\n",
    "                         synsets_similarity(lemmas1, lemmas2, \"wup\"),\n",
    "                         #synsets_similarity(lemmas1, lemmas2, \"lin\"),\n",
    "                         #same_num_entities(ne1, ne2, \"PERSON\"),\n",
    "                         #same_num_entities(ne1, ne2, \"ORGANIZATION\"),\n",
    "                         #same_num_entities(ne1, ne2, \"LOCATION\"),\n",
    "                         #same_num_entities(ne1, ne2, \"GPE\"),\n",
    "                         #same_num_entities(ne1, ne2, \"FACILITY\"),\n",
    "                         #lesk_similarity(lemmas1, lemmas2),\n",
    "                         #sentiment_similarity(lemmas1, lemmas2),\n",
    "                         length_difference(all_words1, all_words2),\n",
    "                         length_difference(lemmas1, lemmas2),\n",
    "                         #unigram_similarity(lemmas1, lemmas2),\n",
    "                         #unigram_similarity(all_words1, all_words2),\n",
    "                         unigram_similarity(words1, words2),\n",
    "                         unigram_similarity_importance(words1, words2),\n",
    "                         #unigram_similarity_importance(lemmas1, lemmas2),\n",
    "                         #unigram_similarity_importance(all_words1, all_words2),\n",
    "                         bigram_similarity(words1, words2),\n",
    "                         #bigram_similarity(lemmas1, lemmas2),\n",
    "                         #bigram_similarity(all_words1, all_words2),\n",
    "                         trigram_similarity(words1, words2),\n",
    "                         #trigram_similarity(lemmas1, lemmas2),\n",
    "                         #trigram_similarity(all_words1, all_words2),\n",
    "                         sentence_emb_similarity(sentence1, sentence2)\n",
    "                        ])\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the main code of the notebook where we call all the previous functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTE TRAIN AND TEST FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fists we load the train and test sentences, the word importance dictionary and we compute the features of train and test. We also scale them before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train sentences\n",
    "train_sentences1, train_sentences2 = get_sentences('train/STS.input.*')\n",
    "\n",
    "# Get importance of each word in train corpus\n",
    "word_importance = get_word_importance(train_sentences1 + train_sentences2)\n",
    "max_importance = max(word_importance.values())\n",
    "min_importance= min(word_importance.values())\n",
    "\n",
    "# Load infersent\n",
    "#infersent = load_infersent(train_sentences1 + train_sentences2)\n",
    "\n",
    "# Get train features and gs\n",
    "features_train = get_features(train_sentences1, train_sentences2)\n",
    "gs_train = get_gs('train/STS.gs.*')\n",
    "\n",
    "# Scale train features\n",
    "scaler = sklearn.preprocessing.StandardScaler();\n",
    "scaler.fit(features_train);\n",
    "features_train_scaled = scaler.transform(features_train)\n",
    "\n",
    "# Get test sentences, features and gs\n",
    "test_sentences1, test_sentences2 = get_sentences('test-gold/STS.input.*')\n",
    "features_test = get_features(test_sentences1, test_sentences2)  \n",
    "gs_test = get_gs('test-gold/STS.gs.*')\n",
    "\n",
    "# Scale test features\n",
    "features_test_scaled = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after computing and scaling the features we train our model and predict with test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVR\n",
    "svr = SVR(kernel = 'rbf', gamma = 0.01, C = 10, epsilon = 0.75, tol = 1)\n",
    "svr.fit(features_train_scaled, gs_train)\n",
    "\n",
    "# Predict\n",
    "test_predict = svr.predict(features_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtain the pearson correlation with the predicted results of our system and the gold-standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.8246006627189321\n"
     ]
    }
   ],
   "source": [
    "correlation = pearsonr(test_predict, gs_test)[0]\n",
    "print(\"Pearson correlation:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case we only added one feature, that computes the euclidean distance between the encodes of sentences given by InferSent. To build the InferSent we have used the 1000000 most common words of English and the words in our train sentences.\n",
    "\n",
    "After adding the feature we saw that the importance of our features reduced a lot. However, removing all of them and leaving only the InferSent feature gives us a correlation of **75.5%**, worst than our previous system, were we obtained a **78.02%**.\n",
    "\n",
    "For that reason we tried to use InferSent with some of our features, and when doing that, we are able to obtain a very high pearson correlation: **82.46%**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
