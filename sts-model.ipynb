{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IHLT PROJECT - SEMANTIC TEXTUAL SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students:\n",
    "   - Albert Rial\n",
    "   - Utku Ünal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we have done a task included in the SemEval (Semantic Evaluation Exercises), a series of workshops which have the main aim of the evaluation and comparision of semantic analysis systems.\n",
    "\n",
    "The task done has been Semantic Textual Similarity (STS), also known as paraphrases detection. A paraphrase between two sentences or texts is when both have the same meaning using different words. The task consists in given two pair of sentences, provide a similarity value between them.\n",
    "\n",
    "To implement our system we have done several tasks:   \n",
    "1) First we have implemented functions needed to load the train and test data and preprocess the sentences.   \n",
    "2) Then we have defined features between sentences in order to describe its similarity or difference.   \n",
    "3) Finally we have used a Support Vector Regression in order to train our system with the features defined and predict the similarity of test sentences.   \n",
    "\n",
    "*Note*: in this project we have tried to avoid the use of pre-trained models and more datasets than the ones given in the competition and used in the subject. We have used the concepts and techniques learnt in class and we think that we have obtained very good results with them. However, in order to increase the accuracy and try to overpass the team in the first position of SemEval 2012, we have done a similar version of this project that, in addition of our features, it also uses a pre-trained model of sentence embeddings called InferSent. Is a model implemented by Facebook that provides semantic representations for English sentences. This system can be found in the other notebook.\n",
    "\n",
    "In this notebook you will find first, our solution implemented and commented, and in the end the details of the process followed and our conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import sentiwordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('words')\n",
    "#nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA AND PREPROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define the functions that load the data (sentences and gold-standard) from the files and preprocess them in order to tokenize the words, remove punctuation and stopwords, get the lemmas, etc. These functions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_gs**: reads the gold-standard scores from a set of files and returns a list with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gs(file):\n",
    "    gs = []\n",
    "    gs_files = glob.glob(file)\n",
    "    for name in gs_files:\n",
    "        with open(name, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                score = line.strip().split('\\t')\n",
    "                gs.append(float(score[0]))\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_sentences**: reads the pairs of sentences from a set of files and returns two lists with the sentences. The first list contains the first sentence of each pair and the second list the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "    sentences1 = []\n",
    "    sentences2 = []\n",
    "    \n",
    "    input_files = glob.glob(file)\n",
    "    \n",
    "    for name in input_files:\n",
    "        with open(name, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                # Split\n",
    "                pair_of_sentences = line.strip().split('\\t')\n",
    "\n",
    "                # Tokenize\n",
    "                sentences1.append(pair_of_sentences[0])\n",
    "                sentences2.append(pair_of_sentences[1])\n",
    "\n",
    "    return sentences1, sentences2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_all_words**: given a sentence, returns its words in lower case, without taking into account the following punctuation characters !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(sentence):\n",
    "    return [word.lower() for word in nltk.word_tokenize(sentence) if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_words**: like the previous function, given a sentence, returns its words in lower case. In this case apart from removing the punctuation we also remove the english stopwords (‘the’, ‘is’, ‘are’, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sentence):\n",
    "    return [word.lower() for word in nltk.word_tokenize(sentence) if word not in string.punctuation and word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_tag**: function that transform a POS tag given by the NLTK POS-tagger to the format that WordNetLemmatizer and Lesk function can undersantd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tag(tag):\n",
    "    if tag[0] in {'N', 'V', 'R'}:\n",
    "        return tag[0].lower()\n",
    "    elif tag[0] in {'J'}:\n",
    "        return 'a'\n",
    "    else:\n",
    "        return tag[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_lemmas**: given a list with pairs of (word, pos_tag) it returns a list with the lemmas of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(pos_tags):\n",
    "    lemmas = []\n",
    "    \n",
    "    wnl = WordNetLemmatizer()\n",
    "    \n",
    "    for pos_tag in pos_tags:\n",
    "        word = pos_tag[0].lower()\n",
    "        tag = transform_tag(pos_tag[1])\n",
    "        \n",
    "        if tag in {'n', 'v', 'r', 'a'}:\n",
    "            lemmas.append(wnl.lemmatize(word, pos=tag))\n",
    "        else:\n",
    "            lemmas.append(word)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_word_importance**: given a list of sentences computes the frequency of each word in all the sentences and returns a dictionary with the importance of each word (considering as importance, the total number of words divided by the frequency of each word). We consider that the words with more meaning are the ones that appear less in the corpus. We use the log in the division in order to have numbers of less magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_importance(sentences):\n",
    "    freq = FreqDist()\n",
    "    total_freq = 0\n",
    "    \n",
    "    \n",
    "    for sentence in sentences:\n",
    "        all_words = get_all_words(sentence)\n",
    "        \n",
    "        for word in all_words:\n",
    "            # Sum 1 to the freq of the word \n",
    "            freq[word.lower()] += 1\n",
    "            \n",
    "            # Sum 1 to the total nb of words\n",
    "            total_freq += 1\n",
    "                    \n",
    "    importance = {}\n",
    "    for word in freq.keys():\n",
    "        importance[word] = math.log(float(total_freq) / float(freq[word]))\n",
    "                    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define all our features. We have several functions that given two sentences, words or lemmas return a float representing a feature of similarity between them. We have both lexical and syntactic features, however not all of them have finally been used because of its performance.\n",
    "The features defined are the ones below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**jaccard_similarity**: given two lists of words the function computes the jaccard similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(words1, words2):\n",
    "    return 1-jaccard_distance(set(words1), set(words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cosine_similarity**: given two sets of words we compute the cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(words1_set, words2_set):\n",
    "    l1 =[];l2 =[] \n",
    "    rvector = words1_set.union(words2_set)  \n",
    "    for w in rvector: \n",
    "        if w in words1_set: l1.append(1)\n",
    "        else: l1.append(0) \n",
    "        if w in words2_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "    # Cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**synsets_similarity**: in this function first, we receive two lists of lemmas and a method for computing similarity between synsets (path, lch, wup, lin). For each lemma in the first list, we calculate the maximum synset similarity between all the lemmas in the second list. With the maximum similarites obtained we compute the mean similarity between the two lists of lemmas. Moreover, as computing the wordnet similarities between one lemma in respect to other is not the same as doing it  inversely, we compute two means.\n",
    "\n",
    "As one lemma has more than one synsets, to compute the synset similarity between one lemma and another we use the function *max_similarity_synsets*. This function receives two lemmas, gets the synsets of each one of them, and for each synset of one lemma computes the similarity between the synsets of the other lemma and returns the maximum similarity. To compute the similarity between synsets it uses *wordnet_similarity*, that is a simple function that calls the wordnet similarity functions given two synsets and the specific method to use.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary where we store the similarity between synsets in order to reduce computational cost\n",
    "computed_synsets_sim = {}\n",
    "\n",
    "def wordnet_similarity(s1, s2, method):\n",
    "    if method == \"path\" and s1 is not None and s2 is not None:\n",
    "        return s1.path_similarity(s2)\n",
    "    \n",
    "    elif method == \"lch\" and s1 is not None and s2 is not None and s1.pos == s2.pos:\n",
    "        return s1.lch_similarity(s2)\n",
    "    \n",
    "    elif method == \"wup\" and s1 is not None and s2 is not None:\n",
    "        return s1.wup_similarity(s2)\n",
    "    \n",
    "    elif method == \"lin\" and s1 is not None and s2 is not None and s1.pos == s2.pos and s1.pos in {'n', 'v', 'r', 'a'}:\n",
    "        return s1.lin_similarity(s2)\n",
    "    \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def max_similarity_synsets(l1, l2, method):\n",
    "    # If are the same we return the max value\n",
    "    if l1 == l2:\n",
    "        if method == \"lch\":\n",
    "            return 3\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    # If we have computed before the similarity we don't compute anything\n",
    "    elif (l1,l2,method) in computed_synsets_sim:\n",
    "        return computed_synsets_sim[(l1,l2,method)]\n",
    "    \n",
    "    # Get synsets\n",
    "    synsets1 = wordnet.synsets(l1)\n",
    "    synsets2 = wordnet.synsets(l2)\n",
    "    \n",
    "    similarities = []\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            # Get the similarity between synsets\n",
    "            similarity = wordnet_similarity(s1, s2, method)\n",
    "            if similarity is not None:\n",
    "                similarities.append(similarity)\n",
    "    \n",
    "    # Return the maximum similarity\n",
    "    if len(similarities) > 0:\n",
    "        computed_synsets_sim[(l1,l2,method)] = max(similarities)\n",
    "        return max(similarities)\n",
    "    else:\n",
    "        computed_synsets_sim[(l1,l2,method)] = 0\n",
    "        return 0\n",
    "\n",
    "def synsets_similarity(lemmas1, lemmas2, method):\n",
    "    sum_sim1 = 0\n",
    "    for l1 in lemmas1:\n",
    "        sum_sim1 += max([max_similarity_synsets(l1, l2, method) for l2 in lemmas2])\n",
    "    mean_sim1 = sum_sim1 / len(lemmas1)\n",
    "    \n",
    "    sum_sim2 = 0\n",
    "    for l2 in lemmas2:\n",
    "        sum_sim2 += max([max_similarity_synsets(l2, l1, method) for l1 in lemmas1])\n",
    "    mean_sim2 = sum_sim2 / len(lemmas2)\n",
    "    \n",
    "    if mean_sim1 > 0 or mean_sim2 > 0:\n",
    "        return (2 * mean_sim1 * mean_sim2)/(mean_sim1+mean_sim2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**same_num_entities**: this is a very basic function that, given two lists of name entities, counts for each list the number of entities of an specific label and returns 1 if both have the same number of them, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_num_entities(ne1, ne2, entity):\n",
    "    num1 = 0 \n",
    "    for p1 in ne1:\n",
    "        if isinstance(p1, nltk.tree.Tree) and p1.label()==entity:\n",
    "            num1 += 1\n",
    "            \n",
    "    num2 = 0    \n",
    "    for p2 in ne2:\n",
    "        if isinstance(p2, nltk.tree.Tree) and p2.label()==entity:\n",
    "            num2 += 1\n",
    "        \n",
    "    if num1 == num2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sentiment_similarity**: given two lists of lemmas it computes the polarity of each list summing the polarity of each lemma, and then computes the absolute difference between them (it is normalized using the maximum polarity). To calculate the polarity of each lemma uses the function *get_sentiment_score*, which uses the SentiWordnet to get the positive score and negative score of each synset of the lemma, and sum all to get the polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(lemma):\n",
    "    synsets = wordnet.synsets(lemma)\n",
    "    score = 0\n",
    "    for s in synsets:\n",
    "        senti_synset = sentiwordnet.senti_synset(s.name())\n",
    "        if senti_synset is not None:\n",
    "            score += senti_synset.pos_score() - senti_synset.neg_score()\n",
    "    return score\n",
    "    \n",
    "def sentiment_similarity(lemmas1, lemmas2):\n",
    "    polarity1 = 0\n",
    "    for l1 in lemmas1:\n",
    "        polarity1 += get_sentiment_score(l1)\n",
    "        \n",
    "    polarity2 = 0\n",
    "    for l2 in lemmas2:\n",
    "        polarity2 += get_sentiment_score(l2)\n",
    "    \n",
    "    if polarity1 > 0 or polarity2 > 0:\n",
    "        return abs(polarity1-polarity2) / max(polarity1, polarity2)\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lesk_similarity**: given two lists of words we apply lesk algorithm to do word sense disambiguation, and we compute the jaccard similarity between the senses obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_similarity(words1, words2):\n",
    "    pos_tags1 = pos_tag(words1)\n",
    "    pos_tags2 = pos_tag(words2)\n",
    "    \n",
    "    lesk_synsets1 = []\n",
    "    for i in range(0, len(words1)):\n",
    "        if(pos_tags1[i] in {'n', 'v', 'r', 'a'}):\n",
    "            lesk_synsets1.append(lesk(words1, words1[i], pos_tags1[i]))\n",
    "            \n",
    "    lesk_synsets2 = []\n",
    "    for i in range(0, len(words2)):\n",
    "        if(pos_tags2[i] in {'n', 'v', 'r', 'a'}):\n",
    "            lesk_synsets2.append(lesk(words2, words2[i], pos_tags2[i]))\n",
    "    \n",
    "    if len(lesk_synsets1) > 0 and len(lesk_synsets2) > 0:\n",
    "        return 1-jaccard_distance(set(lesk_synsets1), set(lesk_synsets2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**length_difference**: given two list of words it returns the difference between their lengths (normalized using the maximum length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_difference(words1, words2):\n",
    "    return abs(len(words1)-len(words2)) / max(len(words1), len(words2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unigram_similarity**: given two lists of words it counts the number of same words that we have in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_similarity(words1, words2):\n",
    "    count_same = 0\n",
    "    for w in words1:\n",
    "        count_same += min(words1.count(w), words2.count(w))\n",
    "    \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count_same/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unigram_similarity_importance**: given two lists of words it counts the number of same words that we have in both lists but when counting a word we take into account its importance in the corpus. This important is computed using previous function *get_word_importance* and stored in the dictionary *word_importance*. The less a word appears in the corpus the more important we consider it, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_similarity_importance(words1, words2):\n",
    "    count_same = 0\n",
    "    for w in words1:\n",
    "        # Count number of same words\n",
    "        count_same += min(words1.count(w), words2.count(w)) * word_importance.get(w, max_importance)\n",
    "        \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count_same/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bigram_similarity**: given two lists of words this function counts the number of same bigrams that we have in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_similarity(words1, words2):\n",
    "    finder1 = BigramCollocationFinder.from_words(words1)\n",
    "    finder2 = BigramCollocationFinder.from_words(words2)\n",
    "    \n",
    "    # We get the bigrams of first sentence and its frequency\n",
    "    bigrams1 = []\n",
    "    freq1 = []\n",
    "    for b1 in finder1.ngram_fd.items():\n",
    "        bigrams1.append(b1[0])\n",
    "        freq1.append(b1[1])\n",
    "    \n",
    "    # We get the bigrams of second sentence and its frequency\n",
    "    bigrams2 = []\n",
    "    freq2 = []\n",
    "    for b2 in finder2.ngram_fd.items():\n",
    "        bigrams2.append(b2[0])\n",
    "        freq2.append(b2[1])\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(bigrams1)):\n",
    "        if bigrams1[i] in bigrams2:\n",
    "            # Count number of same bigrams\n",
    "            count += min(freq1[i], freq2[bigrams2.index(bigrams1[i])])\n",
    "            \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trigram_similarity**: given two lists of words this function counts the number of same trigrams that we have in both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_similarity(words1, words2):\n",
    "    finder1 = TrigramCollocationFinder.from_words(words1)\n",
    "    finder2 = TrigramCollocationFinder.from_words(words2)\n",
    "    \n",
    "    # We get the bigrams of first sentence and its frequency\n",
    "    trigrams1 = []\n",
    "    freq1 = []\n",
    "    for t1 in finder1.ngram_fd.items():\n",
    "        trigrams1.append(t1[0])\n",
    "        freq1.append(t1[1])\n",
    "    \n",
    "    # We get the trigrams of second sentence and its frequency\n",
    "    trigrams2 = []\n",
    "    freq2 = []\n",
    "    for t2 in finder2.ngram_fd.items():\n",
    "        trigrams2.append(t2[0])\n",
    "        freq2.append(t2[1])\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(trigrams1)):\n",
    "        if trigrams1[i] in trigrams2:\n",
    "            # Count number of same trigrams\n",
    "            count += min(freq1[i], freq2[trigrams2.index(trigrams1[i])])\n",
    "            \n",
    "    if len(words1) > 0 or len(words2) > 0:\n",
    "        return 2*count/(len(words1)+len(words2))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call previous functions to preprocess sentences and compute features\n",
    "In the following function we receive two sentences and using the first functions defined in this notebook we preprocess them, in order to get all words, words without stopwords, POS tags, lemmas and NEs. Once preprocessed, we compute the features between each pair of sentences and we append the features to our features array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(sentences1, sentences2):\n",
    "    features = []\n",
    "    for i in range(len(sentences1)):\n",
    "        sentence1 = sentences1[i]\n",
    "        sentence2 = sentences2[i]\n",
    "        \n",
    "        # Get all words\n",
    "        all_words1 = get_all_words(sentence1)\n",
    "        all_words2 = get_all_words(sentence2)\n",
    "        \n",
    "        # Get words without stopwords\n",
    "        words1 = get_words(sentence1)\n",
    "        words2 = get_words(sentence2)\n",
    "        \n",
    "        # POS tags\n",
    "        pos_tags1 = pos_tag(words1)\n",
    "        pos_tags2 = pos_tag(words2)\n",
    "\n",
    "        # Lemmas\n",
    "        lemmas1 = get_lemmas(pos_tags1)\n",
    "        lemmas2 = get_lemmas(pos_tags2)\n",
    "\n",
    "        # Name entities\n",
    "        ne1 = ne_chunk(pos_tags1)\n",
    "        ne2 = ne_chunk(pos_tags2)\n",
    "        \n",
    "        # Features\n",
    "        features.append([jaccard_similarity(lemmas1, lemmas2),\n",
    "                         jaccard_similarity(words1, words2),\n",
    "                         jaccard_similarity(all_words1, all_words2),\n",
    "                         #cosine_similarity(set(words1),set(words2)),\n",
    "                         #cosine_similarity(set(all_words1),set(all_words2)),\n",
    "                         #cosine_similarity(set(lemmas1),set(lemmas2)),\n",
    "                         synsets_similarity(lemmas1, lemmas2, \"lch\"),\n",
    "                         synsets_similarity(lemmas1, lemmas2, \"path\"),\n",
    "                         synsets_similarity(lemmas1, lemmas2, \"wup\"),\n",
    "                         synsets_similarity(lemmas1, lemmas2, \"lin\"),\n",
    "                         #same_num_entities(ne1, ne2, \"PERSON\"),\n",
    "                         #same_num_entities(ne1, ne2, \"ORGANIZATION\"),\n",
    "                         #same_num_entities(ne1, ne2, \"LOCATION\"),\n",
    "                         #same_num_entities(ne1, ne2, \"GPE\"),\n",
    "                         #same_num_entities(ne1, ne2, \"FACILITY\"),\n",
    "                         #lesk_similarity(lemmas1, lemmas2),\n",
    "                         #sentiment_similarity(lemmas1, lemmas2),\n",
    "                         length_difference(all_words1, all_words2),\n",
    "                         length_difference(lemmas1, lemmas2),\n",
    "                         unigram_similarity(lemmas1, lemmas2),\n",
    "                         unigram_similarity(all_words1, all_words2),\n",
    "                         unigram_similarity(words1, words2),\n",
    "                         unigram_similarity_importance(words1, words2),\n",
    "                         unigram_similarity_importance(lemmas1, lemmas2),\n",
    "                         unigram_similarity_importance(all_words1, all_words2),\n",
    "                         bigram_similarity(words1, words2),\n",
    "                         bigram_similarity(lemmas1, lemmas2),\n",
    "                         bigram_similarity(all_words1, all_words2),\n",
    "                         trigram_similarity(words1, words2),\n",
    "                         trigram_similarity(lemmas1, lemmas2),\n",
    "                         trigram_similarity(all_words1, all_words2)\n",
    "                        ])\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the main code of the notebook where we call all the previous functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTE TRAIN AND TEST FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fists we load the train and test sentences, the word importance dictionary and we compute the features of train and test. We also scale them before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train sentences\n",
    "train_sentences1, train_sentences2 = get_sentences('train/STS.input.*')\n",
    "\n",
    "# Get importance of each word in train corpus\n",
    "word_importance = get_word_importance(train_sentences1 + train_sentences2)\n",
    "max_importance = max(word_importance.values())\n",
    "min_importance= min(word_importance.values())\n",
    "\n",
    "# Get train features and gs\n",
    "features_train = get_features(train_sentences1, train_sentences2)\n",
    "gs_train = get_gs('train/STS.gs.*')\n",
    "\n",
    "# Scale train features\n",
    "scaler = sklearn.preprocessing.StandardScaler();\n",
    "scaler.fit(features_train);\n",
    "features_train_scaled = scaler.transform(features_train)\n",
    "\n",
    "# Get test sentences, features and gs\n",
    "test_sentences1, test_sentences2 = get_sentences('test-gold/STS.input.*')\n",
    "features_test = get_features(test_sentences1, test_sentences2)  \n",
    "gs_test = get_gs('test-gold/STS.gs.*')\n",
    "\n",
    "# Scale test features\n",
    "features_test_scaled = scaler.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN AND PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, after computing and scaling the features we train our model and predict with test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVR\n",
    "svr = SVR(kernel = 'rbf', gamma = 0.01, C = 100, epsilon = 0.75, tol = 1)\n",
    "svr.fit(features_train_scaled, gs_train)\n",
    "\n",
    "# Predict\n",
    "test_predict = svr.predict(features_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtain the pearson correlation with the predicted results of our system and the gold-standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: 0.7801798091320731\n"
     ]
    }
   ],
   "source": [
    "correlation = pearsonr(test_predict, gs_test)[0]\n",
    "print(\"Pearson correlation:\", correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS AND CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS\n",
    "\n",
    "First we implemented the functions needed to load the train and test data and preprocess the sentences. \n",
    "\n",
    "After that we started defining features between sentences. At the beginning we had very simple ones:   \n",
    "- Jaccard similarity between lemmas and words.\n",
    "- Cosine similarity between lemmas and words.\n",
    "- LCH similarity between synsets.\n",
    "\n",
    "With three features defined, we started thinking about what we could use in order to train our system. First, we thought about using a CNN (Convolutional Neural Network) but after analyzing the train dataset we saw that maybe it was too small to use that kind of model. For that reason, we decided to implement a linear regression using Support Vector Regression (sklearn.svm.SVR).\n",
    "\n",
    "With those three features and a linear regression we obtained a pearson correlation of **61.8%** between gold-standard and our results.\n",
    "\n",
    "The next features defined were:\n",
    "- Count if sentences had the same number of entities of the same kind (PERSON, ORGANIZATION, LOCATION, ...)\n",
    "- Length difference\n",
    "- Sentiment polarity similarity\n",
    "- Lesk synsets similarity\n",
    "\n",
    "When adding these features we saw an improvement and we obtained a pearson correlation of **63.2%**. Then we evaluated which features were the ones that were more important in our dataset and we saw that the most important were the Jaccard similarity between Lemmas and the LCH similarity between Synsets. For that reason we decided to try to implement similarity between Synsets but using another method:\n",
    "- Path similarity between synsets.\n",
    "\n",
    "In this point, we did not only add these feature. We also started trying different parameters of the Support Vector Regression model. We obtained the best values when using the *rbf* kernel instead of *linear*, so we changed it. We also changed the kernel coefficient (gamma), the regularization parameter (C), the epsilon (epsilon) and the tolerance for stopping criterion (tol).\n",
    "\n",
    "Changing those values and with the new feature added, we obtained a higher pearson correlation: **66.4%**.\n",
    "\n",
    "After that, we analyzed in which type of sentences we were failing most and we saw that we were failing in sentences that shared some trigrams and bigrams. For that reason we implemented the following features:\n",
    "- Unigram similarity (number of same unigrams/words that both sentences share)\n",
    "- Bigram similarity (number of same bigrams that both sentences share)\n",
    "- Trigram similarity (number of same trigrams that both sentences share)\n",
    "\n",
    "Using those features we obtained a pearson correlation of **68.2%**. However, we thought that when comparing words in the unigram similarity feature, we could take into account how significant is the word we are comparing. We consider the approach that a word is more significant than other if it appears less in the corpus. For that reason, we computed the importance of each word in the training corpus as the total number of words divided by the frequency of that word. This importance is used in the following feature:\n",
    "- Unigram similarity giving more weight to the important unigrams/words.\n",
    "\n",
    "With this feature we obtained a higher pearson correlation: **70.6%**. However, at this point we tested again different parameters and obtained a pearson correlation of **73.6%**.\n",
    "\n",
    "Finally to improve that pearson correlation we tried to use more similarity methods between Synsets. We did the ones left:\n",
    "- Lin similarity between synsets\n",
    "- Wup similarity between synsets\n",
    "\n",
    "With these last features we obtained a pearson correlation of **75.7%**, overpassing the mark of the participant at position 10th in SemEval 2012.\n",
    "\n",
    "After getting this result we thought that we could try to remove some features and try different combinations in order to obtain even a better result. In fact, analyzing the importance of each feature by removing them one by one, we saw that there were some features we could remove, as we were obtaining better results without them. These features are:\n",
    "- Cosine similarity between lemmas and words.\n",
    "- Count if sentences had the same number of entities of the same kind (PERSON, ORGANIZATION, LOCATION, ...)\n",
    "- Length difference\n",
    "- Sentiment polarity similarity\n",
    "- Lesk synsets similarity\n",
    "\n",
    "Removing those features and tuning the parameters of the SVR we finally obtained a pearson of: **78.02%**.\n",
    "\n",
    "\n",
    "### CONCLUSIONS\n",
    "The best result obtained without using the pre-trained model InferSent is **78.02%**. We think it is a very good result as we would overpass the 8th position in the SemEval competition and we would be close to the result obtained of the 5th participant. In addition, we have to take into account that we use very basic techniques and without more data than the one provided by the competition. However, a disadvantage of our solution is that the computational cost is quite high. This is because of the features that compute the similarity between synsets, where we have a 4-inner loop when comparing all synsets of all lemmas between the two sentences. We have tried to reduce the computational cost by comparing less synsets, but the resulst obtained were worst. For that reason, and as in this competition there is no efficiency requirement, we have done the comparision with all synsets.\n",
    "\n",
    "In the other notebook you can see the result obtained when using InferSent with some of our features. In that notebook we have included some comments of that result. We advance that with only using InferSent, you get worst results. Nevertheless, when using InferSent combined with our system we have been able to obtain a pearson correlation of **82.46%**, which overpasses the result of the winner of the SemEval 2012 competition.\n",
    "\n",
    "Personally, we think that this has been a very good project for us as we have been able to apply most of the concepts and techniques learnt in the subject. In fact, only using things seen and made in the subject we have been able to get a very good result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
